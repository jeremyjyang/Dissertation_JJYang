\chapter{Introduction and Background}

What is the strongest biomedical evidence about a disease for discovery of novel pharmaceutical therapies? This is a fundamental challenge for biomedical scientists, but also directly translates to a parallel question for informatics and data science: Can we systematically assemble and query biomedical heterogeneous knowledge graphs in a computational discovery platform guided by rational, algorithmic measures of relevance and confidence, facilitating scientific discovery? And, how have continuing waves of scientific and technological progress informed and empowered these inquiries? 

The research described herein consists of several distinct projects unified by this common theme. The three main projects are (1) Badapple: Bioassay data associative promiscuity prediction learning engine, (2) KGAP: Knowledge graph analytics platform, and (3) TIGA: Target illumination GWAS analytics. 

Badapple employs empirical bioassay data from PubChem and the Molecular Libraries Program to recognize patterns of promiscuity (non-selectivity), associated with molecular scaffolds. KGAP combines data from two NIH programs, LINCS (Library of integrated network-based cell signatures)  and IDG (Illuminating the druggable genome) to generate and evaluate hypotheses for novel drug targets. TIGA processes data from the NHGRI-EBI GWAS Catalog to aggregate experimental genomic variant to trait associations as novel drug target hypotheses.  Badapple was published in 2016. Both KGAP and TIGA have led to publications in 2021 (KGAP in 2nd minor revision review, TIGA published 6/4/21). Three additional projects (all with published papers) are also described, for which the author contributed in a non-leading but major role, each different but reinforcing the common theme, that scientific discovery is empowered by rational, algorithmic, semantic, domain-aware assembly and querying of knowledge graphs (or knowledge-graph-ready datasets). 

This dissertation is closely related to the publications resulting from these projects, and many sections are adaptations of publication sections. By combining these contributions in the context of their common themes and approach, the intended benefit is to facilitate application of this approach to additional problem areas. 

\section{Foundations}

This line of investigation builds upon the prior contributions of Wild, Ding, Oprea and others, particularly Chem2Bio2RDF\cite{Chen2010-to}, SLAP\cite{Chen2012-iq}, and IDG\cite{Oprea2018-cp}. These influential resources have contributed to the advancement of systems biology through informatics approaches. Despite progress many challenges remain, for (1) foundational biomedical sciences and understanding complex processes of molecular physiology and pathology, and (2) informatics and data science, to develop effective systems for biomedical knowledge representation, processing, and analysis.  The term "big data" is vague but is a useful shorthand for the problems of data volume, velocity, variety and veracity, which must be addressed in data-intensive areas such as biomedicine.  A centrally important consequence is that access to big data is insufficient, and to be useful, manageable subsets must be searchable and organized for focused analysis.  A simple but profound example of this is Google's initial PageRank algorithm, game-changing for web search, by allowing hundreds of thousands of hits to be manageable by ranking with sufficient relevance for human utility and acceptance.  This research has been motivated by observed use cases where drug discovery scientists focused on a specialized disease area seek to identify and harness the relevant, reliable knowledge to both develop an improved understanding of the underlying biology and explore new hypotheses for drug therapies, whether by novel or repurposed compounds. Key examples are (1) prioritizing hit compounds from a high throughput bioassay, and (2) prioritizing gene hypotheses from GWAS and gene expression datasets.

What types of knowledge are relevant to biomedical and pharmaceutical research?  A wide variety, which may be termed the translational spectrum, from basic science to clinical research and observational data. Given that discoveries build upon knowledge, with stronger knowledge increasing chances for discovery, what are the best measures for confidence?  Confidence evaluation is an essential aspect of any procedure for converting raw data to knowledge.  Assigning confidence levels may also be equivalent to prediction, but in some ways is superior.  Whereas an ML approach may generate a model regardless of the data quality, a confidence level approach should be able to identify cases where the appropriate conclusion is none, an important result in science. Knowledge graphs are well suited to represent human knowledge and provide a platform for research applications. By incorporating measures of confidence and relevance, to evaluate evidence, knowledge graphs can empower and augment human cognition for scientific discovery.

\section{Research area: informatics and data science}

Given the rapid emergence and evolution of informatics and its sub-disciplines, and the newer related designation "data science", a research contribution such as this should be classified in this context to facilitate its interpretation and evaluation. Briefly stated, this work employs methods from bioinformatics and cheminformatics, but its main contributions are best classified as in the realm of data science. Regarding the definition of data science, several have been proposed\cite{ONeil2013-je,Peng2016-gq}, but there is a broad agreement that data science is interdisciplinary and focused on deriving actionable knowledge about the real world. In this work, the application area is pharmaceutical discovery, and the real world intent is to advance medical science and improve human health.

\section{Thesis}

The unifying thesis of this dissertation concerns the development and use of measures of confidence and relevance to evaluate biomedical evidence. This is data science methodology. The research projects described involve distinct and separate sub-domains of the biomedical sciences and informatics, but the approaches and methods are related in several respects centered upon the use of confidence and relevance measures to evaluate evidence. In all projects the use cases are from pharmaceutical discovery, specifically lead and target identification and prioritization.  

\section{Novelty}

The novelty of this dissertation consists of (1) the individual novelty of each individual research project, in aggregate, and (2) the novelty of the common methodological elements, as a general approach to evidence evaluation. Novel methods (e.g. Badapple promiscuity score, TIGA RCRAS citation measure) are presented, and existing methods are employed in novel applications.

\section{Validation}

Validation is an important topic for computational sciences. A predictive algorithm can prove its value by validation against a dataset of trusted results. Methods presented herein are validated against datasets of trusted results, to the extent possible, but this approach is not fully applicable. Algorithms which are primarily descriptive, rather than predictive, in some sense require no validation except confirmation of their accuracy. For example, the Badapple promiscuity score is strictly speaking an aggregate statistic, combining input bioactivity data, though it may be used in a predictive scenario. However, Badapple was validated using a cross-validation approach. For KGAP and TIGA, designed to associate diseases with druggable genes, an additional challenge is the fundamental uncertainties in molecular biology and medicine, and consequent lack of gold-standard disease-gene associations. For example, the most trusted associations of genes to the disease Type-2 Diabetes are fraught with uncertainty as to their role, and the definitions and diagnoses of the disease itself are at issue. Yet, the medical and public health needs are real and great. 

\section{Researcher contributions}

The research projects described in this dissertation have each been team science efforts producing a peer-reviewed publication. This dissertation combines portions of each project for which the Ph. D. candidate has been the major contributor, and specifies those contributions using the Scholarly Contributions and Roles Ontology (SCoRO)\cite{Shotton2020-ph}. Each of these descriptions has been reviewed and approved by the principal investigator (last author for each paper) of the corresponding projects. 