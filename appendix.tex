\begin{appendices}

%Some Table of Contents entry formatting
\addtocontents{toc}{\protect\renewcommand{\protect\cftchappresnum}{\appendixname\space}}
\addtocontents{toc}{\protect\renewcommand{\protect\cftchapnumwidth}{6em}}

%Begin individual appendices, separated as chapters

\chapter{Abbreviations and definitions}

\section{Badapple}

\begin{table}
\caption{Abbreviations (Badapple)}
\begin{tabular}{p{0.2\linewidth}p{0.8\linewidth}}
\hline
\makecell[r]{\textbf{BA}} & batting average \\
\makecell[r]{\textbf{BAO}} & BioAssay Ontology \\
\makecell[r]{\textbf{BARD}} & BioAssay Research Database \\
\makecell[r]{\textbf{HTS}} & high-throughput screening \\
\makecell[r]{\textbf{ML}} & machine learning \\
\makecell[r]{\textbf{MLP}} & Molecular Libraries Program \\
\makecell[r]{\textbf{MLSMR}} & Molecular Libraries Small Molecule Repository \\
\makecell[r]{\textbf{Ro5}} & Rule of 5 \\
\makecell[r]{\textbf{SAR}} & structure–activity relationships \\
\makecell[r]{\textbf{UNMCMD}} & University of New Mexico Center for Molecular Discovery \\
\hline
\end{tabular}
\end{table}

\section{KGAP}

\begin{table}
\caption{Abbreviations (KGAP)}
\begin{tabular}{p{0.2\linewidth}p{0.8\linewidth}}
\hline
\makecell[r]{\textbf{ATC}} & Anatomical Therapeutic Chemical Classification (WHO) \\
\makecell[r]{\textbf{AUROC}} & Area under receiver operator characteristic (ROC) curve \\
\makecell[r]{\textbf{IDG}} & Illuminating the druggable genome (NIH Common Fund) \\
\makecell[r]{\textbf{KG}} & Knowledge graph, also known as a knowledge network \\
\makecell[r]{\textbf{KGAP}} & Knowledge graph analytics platform \\
\makecell[r]{\textbf{LINCS1000}} & "Landmark genes" (approximately 1000) from LINCS, chosen for maximal inference of full genomic expression. \\
\makecell[r]{\textbf{LINCS}} & Library of integrated network-based cellular signatures (NIH Common Fund) \\
\makecell[r]{\textbf{MoA}} & Mechanism of action, describing biomolecular details of drug effect \\
\makecell[r]{\textbf{PD}} & Parkinson's disease \\
\makecell[r]{\textbf{SYNGR3}} & Synaptogyrin-3, a human gene, subject of case study in this paper \\
\makecell[r]{\textbf{TCRD}} & Target central resource database (IDG) \\
\makecell[r]{\textbf{TDL}} & Target development level (IDG) \\
\makecell[r]{\textbf{TIN-X}} & Target Importance and Novelty Explorer (IDG) \\
\hline
\end{tabular}
\end{table}

\section{TIGA}


\begin{table}
\caption{Abbreviations and Definitions (TIGA)}
\begin{tabular}{p{0.2\linewidth}p{0.8\linewidth}}
\hline
\multicolumn{2}{p{1.0\linewidth}}{Common terms used in GWAS and related fields can vary in their definitions and connotations depending on context. Therefore for clarity and rigor the following definitions are provided, which we consider consistent with best practices in the GWAS and drug discovery communities.}\\
\hline
\makecell[r]{\textbf{genotype}} & An organism has one genotype, comprised of a germ line genome and multiple somatic genomes. Statistical models may assume a population distribution hence a population genotype. \\
\makecell[r]{\textbf{phenotype}} & An organism has one phenotype, comprised of (potentially) all non-genomic observable characteristics, a.k.a. phenotypic traits. \\
\makecell[r]{\textbf{gene}} & Genomic unit responsible for an expression product. Protein coding genes are a subset of this definition.  \\
\makecell[r]{\textbf{trait}} & Single non-genomic, observable characteristic. \\
\makecell[r]{\textbf{drug target}} & Biomolecular entity involved in the mechanism of action of a drug. The IDG project is protein-centric; hence drug targets are all proteins. \\
\hline
\end{tabular}
\end{table}



\chapter{Glossary of data science terms}

\begin{singlespace}
\begin{longtable}{p{0.3\linewidth}p{0.7\linewidth}}
\caption{Data science terms: Interpretation}\\
\hline
\makecell[r]{\textbf{Intelligibility}} & Similar to interpretability, can processes or results be understood by humans. However, not all humans understand equally. So perhaps
intelligibiilty is context-dependent. \\
\makecell[r]{\textbf{Evaluation} \\ \textbf{(of interpretation)}} & An interpretation is an intelligible assertion about the world that may be evaluated in various ways, such as usefulness,
or validation by quantifyable proxy. \\
\makecell[r]{\textbf{Explainability}} & Explaining model behavior, why or how a result is generated. Interpretability is necessary but not sufficient for explainability. \\
\makecell[r]{\textbf{Completeness}} & Accuracy of explanations. A simplified, less-complete explanation may be intelligible and pursuasive, but potentially misleading or
unethical. \\
\makecell[r]{\textbf{Contrastive explanation}} & Explaining a result by contrast with a different case. E.g. Why was my application rejected but this other one accepted? \\
\makecell[r]{\textbf{Algorithmic Transparency}} & Opposite of opacity and black-boxes, can the system's processes by investigated. \\
\makecell[r]{\textbf{Algorithmic Fairness}} & Are results unbiased? In some domains could violate legal anti-discrimination protections. In research science, could raise COI
concerns. \\
\makecell[r]{\textbf{Accountability}} & Related to transparency, can methods and results be accounted for algorithmically. E.g. waiter, how was this service charge calculated?
And how was the price of the paella determined from the ingredient list, labor costs, and overhead? \\
\makecell[r]{\textbf{Black-box system}} & Non-transparent, opaque system, not explainable since the specifics of the process for generating a result are inaccessible. \\
\makecell[r]{\textbf{Pursuasive systems}} & Human opinions and decisions can be affected, and systems can be optimized for pursuasiveness, but accuracy may be compromised. \\
\makecell[r]{\textbf{Context-aware systems}} & For HCI issues of Interaction Design and Learnability, context-aware systems connect the application logic to the user and UI
designer.  \\
\makecell[r]{\textbf{iML}} & Interpretable ML \\
\makecell[r]{\textbf{FATML}} & Fair, accountable and transparent ML \\
\makecell[r]{\textbf{XAI}} & Explainable AI, a term favored by US-DARPA. \\
\hline
\end{longtable}
\end{singlespace}

\begin{table}
\caption{Data science terms: Wrangling}
\begin{tabular}{p{0.3\linewidth}p{0.7\linewidth}}
\hline
\makecell[r]{\textbf{Data cleaning}} & Processing of files to fix or remove various kinds of errors and inconsistencies and prepare for analysis.  E.g. numerical values which are
not interpretable as numbers, removal of cases with missing necessary values, values which are obvious errors (e.g. DOB in future).  E.g. converting problematic
character encodings to ASCII or UTF-8. \\
\makecell[r]{\textbf{Dirty data}} & Needs a lot of cleaning, maybe much of the overall effort. \\
\makecell[r]{\textbf{Tidy data}} & Data organized and described well by metadata, such as in tables with well defined columns, and files with standard formats (e.g. CSV). \\
\makecell[r]{\textbf{Data munging}} & Approximately: Various transformations to process data into clean and tidy files.  Converting among formats, parsing multiple fields from
one delimited string, etc. \\
\makecell[r]{\textbf{Noisy data}} & Data may be clean and tidy but the values may reflect much randomness, such as measurement error.  E.g. resting blood pressure measurements
may vary a lot from the "true" value for a patient. \\
\makecell[r]{\textbf{Data wrangling}} & Approximately: Various processes for cleaning, tidying, and transforming data into forms suitable for computation. \\
\makecell[r]{\textbf{Data formats}} & File types used to contain and communicate data, such as JSON, XML, and CSV, but which can vary greatly within these and other general
families. \\
\makecell[r]{\textbf{Extract, Transform} \\ \textbf{and Load (ETL)}} & Common term describing the wrangling workflow for building a database from a data source. \\
\makecell[r]{\textbf{Feature Extraction}} & Some types of raw data such as image or audio lack well defined features, which must be defined based on domain knowledge. \\
\makecell[r]{\textbf{ML-Ready Dataset}} & After all the cleaning and other wrangling is done, and the dataset is an acceptable input file for the ML components, it is said to be
ML-ready.  Many ML examples, tutorials and challenges (e.g. Kaggle) deal with ML-ready data only and ignore or minimize the importance of data wrangling. \\
\hline
\end{tabular}
\end{table}

\begin{table}
\caption{Data science terms: Semantics and metadata}
\begin{tabular}{p{0.3\linewidth}p{0.7\linewidth}}
\hline
\makecell[r]{\textbf{Data dictionary}} & Defines the datatypes in a dataset or database.. \\
\makecell[r]{\textbf{Schema}} & Defines datatypes in a dataset or database, and usually more such as relationships, mappings, primary and foreign keys. \\
\makecell[r]{\textbf{Identifiers}} & Names or numbers used to identify entities. \\
\makecell[r]{\textbf{Entity mapping}} & Associating data fields corresponding to the same entity, but using different systems of identifiers. \\
\makecell[r]{\textbf{Persistent Identifiers}} & Designed to be unchanging, reliable in the future. \\
\makecell[r]{\textbf{URI vs. URL}} & Uniform Resource Identifier are persistent IDs employed in RDF, which may or may not be URLs available on the WWW. \\
\makecell[r]{\textbf{Controlled Vocabulary}} & Set of pre-defined terms for a specific purpose, e.g. disease names defined by the World Health Organization International
Classification of Diseases. \\
\makecell[r]{\textbf{Taxonomy}} & Hierarchical organization of entities, such as the taxonomy of organisms into plants, animals, etc. \\
\makecell[r]{\textbf{Ontology}} & Refers to both an organization of entities and relations and related knowledge, and the study of such knowledge systems. \\
\makecell[r]{\textbf{Semantic Web} \\ \textbf{Technologies}} & A set of community approaches, formats and methods first inspired by Tim Berners Lee in 2001, and organized through the W3C. \\
\makecell[r]{\textbf{RDF}} & Resource Description Framework, a foundation of SWT, wherein all knowledge is represented by semantic triples. \\
\makecell[r]{\textbf{Semantic Triples}} & Subject - predicate - object triples, the unit of knowledge in RDF. \\
\makecell[r]{\textbf{Sparql}} & RDF query language. \\
\makecell[r]{\textbf{Knowledge Representation}} & Refers to the methods and formats of encoding and storing human knowledge, such as by RDF semantic triples, but also the basic
issue in cognitive science which applies to human and artificial intelligence alike: What is knowledge in relation to cognition? \\
\hline
\end{tabular}
\end{table}

\begin{singlespace}
\begin{longtable}{p{0.3\linewidth}p{0.7\linewidth}}
\caption{Data science terms: Statistics}\\
\multicolumn{2}{c}{\emph{(See statistics textbooks and other resources for more authoritative definitions of these terms.)}} \\ 
\hline
\makecell[r]{\textbf{Confidence Interval (CI)}} & Typical 95\% CI means given the sample data and assumptions, the parameter will be within the CI 95\% of the time if the data is
re-sampled. \\
\makecell[r]{\textbf{P-Value}} & Probability of the sample data if the null hypothesis is true. \\
\makecell[r]{\textbf{Probability Distribution}} & For a discrete random variable, gives the probability of any value, for a continuous random variable, gives the probability of
any interval. \\
\makecell[r]{\textbf{Statistic}} & Any formulaic numerical descriptor of a sample (e.g. mean, median). \\
\makecell[r]{\textbf{Parameter}} & The parameters of a normal or Gaussian distribution are the mean and variance.  For any such family, the function can be fully defined by the
parameters.  Parametric statistics is largely focused on estimating population (global) parameters and their uncertainty from sample data. \\
\makecell[r]{\textbf{Multiple Testing}} & If a coin is flipped 5 times, all heads, the coin is likely unfair.  However, if 100 coins are all flipped 5 times, it is highly likely
at least one will be all heads, and that will provide no evidence of unfairness.  In this way the interpretation of P-values and other metrics much be adjusted in
multiple testing scenarios. \\
\makecell[r]{\textbf{Chance Correlation}} & With random variables, correlation may reflect explanatory truths, or simply be due to random chance. \\
\makecell[r]{\textbf{False Discovery Rate}} & A quantification of the probability of false discovery, or chance correlation. (See XKCD comic on M\&M associated risks.) \\
\makecell[r]{\textbf{Confounder}} & When associating two variables, a confounder is another third variable which may be more explanatory.  In an ideally executed randomized
controlled trial, confounders are impossible since only the controlled independent variables differ between the control and test groups. \\
\makecell[r]{\textbf{Independent Variables}} & Random variables which are not correlated, not affecting each other or each affected by another variable. \\
\makecell[r]{\textbf{Correlation versus} \\ \textbf{causation}} & Despite being a fundamental truth of statistics and rationality, an common area of confusion and misinterpretation in
science, and in human cognition more generally. \\
\makecell[r]{\textbf{Random sampling}} & Selecting a representative subset of a population by some random process. \\
\makecell[r]{\textbf{Sampling bias}} & Non-random sampling.  For example, online surveys are biased toward persons who are online and like to respond to surveys, thus not
representative of the general population. \\
\makecell[r]{\textbf{Population}} & Conceptually the global set of entities of interest.  For analytics pertaining to US federal elections, the population would be all US voters.
For anti-depressant drug trials, the population might be all humans diagnosed with major depressive disorder. In a study, a sample is used to make inferences about
the population. \\
\makecell[r]{\textbf{Frequentist versus} \\ \textbf{Bayesian Statistics}} & Among statisticians and statistical scholarship, there are two main schools, frequentist and Bayesian.
Oversimplifying somewhat, these schools don't disagree about math, including Bayes Theorem, but do disagree about interpretations.  Frequentists tend to view
population parameters as truth.  Bayesians tend to view evidence as updating belief. \\
\makecell[r]{\textbf{Bayesian Approach}} & Somewhat beyond the basic truth of Bayes Theorem, a Bayesian approach is often iterative, and can adjust the parameters of a
distribution, as a way of continuous learning from streams of evidence. \\
\makecell[r]{\textbf{Prior Distribution}} & A Bayesian learning iteration begins with belief in a prior distribution, to be adjusted according to the evidence in the sample data.
In a simple univariate case, this involves multiplication by a Bayesian factor.  For a simple coin flip, we would probably believe in a prior of 50-50\% head vs
tail.  Posterior Distribution	After adjustment of the belief represented by the prior distribution, the result of the Bayesian learning iteration is the posterior
distribution.  After flipping our coin and getting 5 heads in a row, our posterior might be 70-30\% head vs tail. \\
\hline
\end{longtable}
\end{singlespace}

\begin{table}
\caption{Data science terms: Visualization}
\begin{tabular}{p{0.3\linewidth}p{0.7\linewidth}}
\hline
\makecell[r]{\textbf{Heatmap}} & Represents a real valued matrix with a color spectrum. \\
\makecell[r]{\textbf{Histogram}} & Represents the sample distribution of a numerical variable, binned into intervals, shown as bars of height proportional to count in the bin. \\
\makecell[r]{\textbf{Bar Chart}} & Heights of bars proportional to some quantity, more general than histogram as the bars may be categorical (e.g. national populations). \\
\makecell[r]{\textbf{Radar Chart}} & Similar to bar chart, but radial. \\
\makecell[r]{\textbf{Scatter Plot}} & Points on 2D (or maybe 3D) axes. \\
\makecell[r]{\textbf{Box Plot}} & Visualizes the distribution of a variable, including the median, 1st and 3rd quartile.  WIth a "box and whiskers" plot, the whiskers may
indicate the 2nd and 98th percentiles. \\
\makecell[r]{\textbf{Visual Channels}} & Informative characteristics, such as color or shape or size. \\
\makecell[r]{\textbf{Pre-Cognitive Awareness}} & Visualization can facilitate cognition by employing this natural pattern recognition capability.  For example, seeing patches of
color in a heatmap is easier than reading and abstracting from all the numbers. \\
\hline
\end{tabular}
\end{table}

\begin{singlespace}
\begin{longtable}{p{0.3\linewidth}p{0.7\linewidth}}
\caption{Data science terms: Machine learning}\\
\hline
\makecell[r]{\textbf{Supervised learning}} & ML involving a training set of labeled cases. \\
\makecell[r]{\textbf{Unsupervised learning}} & ML with no training, detecting patterns as in clustering. \\
\makecell[r]{\textbf{Clustering}} & Grouping of cases, which may be overlapping or disjoint, hierarchical or not, with many possible algorithms. \\
\makecell[r]{\textbf{Validation}} & Assessment of a predictive model versus some asserted truth. \\
\makecell[r]{\textbf{Cross-validation}} & Splitting a dataset into training and test cases, and repeatedly training and testing the resulting models. \\
\makecell[r]{\textbf{Overfitting}} & Training a model to fit the training data, but with sub-optimal applicability to other cases. \\
\makecell[r]{\textbf{Regularization}} & In training a predictive model, a method of reducing overfitting. \\
\makecell[r]{\textbf{Loss function}} & In training a predictive model, a measure of performance loss, to be minimized.  A.k.a. cost function. \\
\makecell[r]{\textbf{Confusion matrix}} & For a predictive classification model, a matrix of true positives, true negatives, false positives, etc., i.e. counts comprising the performance. \\
\makecell[r]{\textbf{True Positives, False} \\ \textbf{Positives, True Negatives,} \\ \textbf{etc.}} & True Positive means predicted true by the model and labeled true in a classification scenario.  Abbreviated TP, FP, TN, FN. \\
\makecell[r]{\textbf{Accuracy}} & (TP + TN) / (P + N).  Equivalently, fraction of correct predictions. \\
\makecell[r]{\textbf{Sensitivity or Recall}} & TP / P \\
\makecell[r]{\textbf{Specificity}} & TN / N \\
\makecell[r]{\textbf{F1 Score}} & 2TP / (2TP + FP + FN) \\
\makecell[r]{\textbf{Classification}} & Predicting non-numerical classes to cases, for example, recognizing a character from an image. \\
\makecell[r]{\textbf{Binary classifier}} & Classifier with two possibilities. \\
\makecell[r]{\textbf{Regression}} & Predicting a numerical endpoint. \\
\makecell[r]{\textbf{Cases}} & Instances of the entity under study, e.g. customers, patients, website sessions, molecules or genes. \\
\makecell[r]{\textbf{Features}} & The variables describing each case. \\
\makecell[r]{\textbf{Endpoints}} & One or more results or observations to be analyzed and/or predicted. \\
\makecell[r]{\textbf{Labels}} & Label generally means a known endpoint, suitable for training. \\
\makecell[r]{\textbf{Vectorization}} & Transformation of nominal features into numeric.  A set of numerical features for a single case is thereby a vector, a.k.a. 1D array or matrix row. \\
\makecell[r]{\textbf{Numerical vs. Categorical} \\ \textbf{data}} & Input data may be numerical or if not, categorical, normally identified by some textual name. \\
\makecell[r]{\textbf{Binary feature}} & True or False, Yes or No, equivalently 1 or 0. \\
\makecell[r]{\textbf{Ordinal data}} & Categorical but ordered, e.g. Low, Medium, High. \\
\makecell[r]{\textbf{Nominal data}} & Non-numerical, textual names. \\
\makecell[r]{\textbf{Training set}} & For a supervised ML workflow, the dataset used to train the predictive model. \\
\makecell[r]{\textbf{Test set}} & For a supervised ML workflow, the dataset used to test the predictive model. \\
\makecell[r]{\textbf{Online learning}} & Whereas many ML scenarios start with gathering a defined dataset, online learning implies a continuous stream of input data.  Thus, the methods applied must be capable of continuous learning. \\
\makecell[r]{\textbf{Predictive model}} & A formal, explicit, method for computing classes or quantities from input cases. \\
\makecell[r]{\textbf{Statistical model}} & The characteristics of the probability distribution of a population feature is one important meaning of this term.  E.g. human weight may be assumed to be normally distributed in the overall population to assess percentiles. \\
\makecell[r]{\textbf{Model selection}} & In ML this may mean attempting several types of predictive model and selecting the best performing. \\
\makecell[r]{\textbf{Feature selection}} & Rational selection of input features to improve performance, or to discard redundant or non-informative features. \\
\makecell[r]{\textbf{Bias and variance}} & In ML bias and variance are terms describing a predictive model which should be balanced to avoid overfitting or underfitting, respectively.  I.e. excess bias means overly biased toward the training set.  Excess variance means the model will vary excessively from the training set. \\
\makecell[r]{\textbf{Parameters}} & In terms of predictive models, the parameters define the model within a given family.  For example a simple 2D linear regression has two parameters, slope and y-intercept.  A neural network has a weight parameter for every input to every node in every layer, input, hidden and output.  In all scenarios, training the model means determining the parameters. \\
\makecell[r]{\textbf{Recommender system}} & A type of predictive model which an entity of some defined output type, to an input case of another type.  E.g. Netflix recommends movies to users.  A sales recommender might recommend prospects for a given product. \\
\makecell[r]{\textbf{Linear Regression}} & Supervised regression method whereby endpoints are formulated as linear sums of input variables weighted by parameter coefficients.  N. B. input variables may be raw features or some transformation (e.g. log or square). \\
\makecell[r]{\textbf{K-Nearest Neighbors} \\ \textbf{Algorithm}} & Supervised classification method based on distances in a space of dimensionality equal to the feature count, and voting by the K nearest neighbors to any case, where K is an adjustable parameter. \\
\makecell[r]{\textbf{K-Means Clustering} \\ \textbf{Algorithm}} & Unsupervised method, based on distances in a space of dimensionality equal to the feature count, where here K is a pre-defined count of clusters to be detected. \\
\makecell[r]{\textbf{Support Vector Machine} \\ \textbf{(SVM) Algorithm}} & Supervised classification method, based on distances in a space of dimensionality equal to the feature count, which generates a decision boundary, by minimizing based on a cost "kernel" function. \\
\makecell[r]{\textbf{Naïve Bayes Algorithm}} & Using Bayes theorem, and assuming independence, training cases can be used to infer conditional probabilities and thereby predict new cases. \\
\makecell[r]{\textbf{Decision Trees Algorithm}} & Also known as Classification and Regression Trees (CART).  At each node in a tree, a decision based on partitioning of one feature is designed to optimize information gain. \\
\makecell[r]{\textbf{Random Forests} \\ \textbf{Algorithm}} & An extension of CART, whereby ensembles (forests) of decision trees are generated with random selection of training cases and features. \\
\makecell[r]{\textbf{Logistic Regression}} & Uses a sigmoid function domain [-Inf, +Inf] and range [0,1] interpretable as a probability and a binary classification.  This is employed as the activation function of each neuron (node) of a neural net. \\
\makecell[r]{\textbf{Artificial Neural Network} \\ \textbf{(ANN) Algorithm}} & Supervised classification method, modeled on a specific conception of biological neurocognition.  The network consists of neurons (a.k.a. nodes) in  one input layer, one output layer, and one or more hidden layers.  Each neuron is represented by a logistic activation function, linear weighted combination of inputs from the preceding layer, and a single output.  Learning is achieved by assessing the error (a.k.a. loss or cost) at the outputs, then back-propagating that error through the layers to optimize the weight parameters.  Overall learning is by iteratively processing errors from many training cases. \\
\makecell[r]{\textbf{Deep Learning}} & Typically refers to ANN with multiple (3+) hidden layers. \\
\makecell[r]{\textbf{Evolutionary Algorithm}} & A.k.a. genetic algorithm.  Involves an intial population, and successive generations selected according to a fitness function, and mutated, possibly by combination and cross-linking of genetic units (genes). \\
\makecell[r]{\textbf{Probabilistic Graph} \\ \textbf{Models}} & Two main types are Bayesian and Markov Networks.  Variables are nodes, and edges represent dependencies, possibly causal. \\
\makecell[r]{\textbf{Agent Based Models}} & Agents a.k.a. cellular automata.  Some classic examples are ant colonies, economic systems, and terrorists.  In contrast to models involving centralized command and control, in agent based models, behavior is an emergent property of the group. \\
\makecell[r]{\textbf{Complex, adaptive, and} \\ \textbf{non-linear systems}} & A large commercial aircraft is a very complicated machine with millions of parts, but these terms refer to a very different kind of complexity.  Whereas the aircraft is designed to behave very precisely and reproducibly for a given set of inputs, according to defined mathematical functions, complex and adaptive systems cannot be described thus.  Agent based modeling is a promising approach.  The overall behavior is said to have emergent properties. \\
\makecell[r]{\textbf{A/B Testing}} & Onlne evaluation of two (or more) options, such as in e-commerce or political solicitation. \\
\makecell[r]{\textbf{Reinforcement Learning}} & With no initial training set, but defined success criteria, the space must be explored, behaviors learned and reinforced by successful outcomes, and successful strategies exploited.   Thus a balance is required between exploration and exploitation.  The prototypical scenario is the "Multi-armed bandit".  \\
\makecell[r]{\textbf{Transfer Learning}} &  \\
\makecell[r]{\textbf{Representation Learning}} &  \\
\makecell[r]{\textbf{Generative Adversarial} \\ \textbf{Network (GAN)}} &  \\
\makecell[r]{\textbf{Autoencoder}} &  \\
\makecell[r]{\textbf{Feature Engineering}} &  \\
\makecell[r]{\textbf{Regularization}} &  \\
\makecell[r]{\textbf{Interpretability}} &  \\
\hline
\end{longtable}
\end{singlespace}

\begin{table}
\caption{Data science terms: Packages and products}
\begin{tabular}{p{0.3\linewidth}p{0.7\linewidth}}
\hline
\makecell[r]{\textbf{Tableau}} & Commercial package for visualization and analysis.   \\
\makecell[r]{\textbf{WEKA}} & Open source ML package for Java \\
\makecell[r]{\textbf{SciKit-Learn}} & Open source ML package for Python \\
\makecell[r]{\textbf{R}} & Open source statistics and analytics framework, with many community packages. \\
\makecell[r]{\textbf{MapReduce}} & Parallelization algorithm invented at Google, designed for big data and large, distributed computing clusters. \\
\makecell[r]{\textbf{Hadoop}} & Apache open source implementation of MapReduce. \\
\makecell[r]{\textbf{TensorFlow}} & Google based open source ANN package. \\
\makecell[r]{\textbf{MATLAB}} & Commercial math and analytics platform.  MAT = matrix.  Many packages available including many for ML. \\
\makecell[r]{\textbf{Octave}} & Open source equivalent to MATLAB, quite code compatible. \\
\makecell[r]{\textbf{KNIME}} & Open source workflow development platform, with many packages. \\
\makecell[r]{\textbf{Plotly}} & Interactive plotting package with APIs for Python, R and more. \\
\makecell[r]{\textbf{Matplotlib}} & Excellent Python plotting package. \\
\hline
\end{tabular}
\end{table}

\begin{table}
\caption{Data science terms: Miscellaneous}
\begin{tabular}{p{0.3\linewidth}p{0.7\linewidth}}
\hline
\makecell[r]{\textbf{Machine Learning}} & Somewhat imprecise term, but generally a set of computational approaches for perceiving patterns or predicting endpoints from data, and
often big data.  Historically ML is associated with computer science, but shares much with statistics and other fields. \\
\makecell[r]{\textbf{Exploratory Data Analysis} \\ \textbf{(EDA)}} & Analysis without specific hypothesis, usually to learn about a dataset, describe, and discover patterns and potential
hypotheses for further investigation. \\
\makecell[r]{\textbf{Data Mining}} & EDA focused on knowledge discovery \\
\makecell[r]{\textbf{Confirmatory Data} \\ \textbf{Analysis}} & Involves a pre-defined hypothesis, tested by the analysis.  A randomized controlled trial is an important example. \\
\makecell[r]{\textbf{Randomized Controlled} \\ \textbf{Trial (RCT)}} & In medicine, a designed, prospective study involving patients randomly assigned to test and control groups, and
normally "double blind" procedures whereby neither the patient nor the clinician administering the treatment knows whether the treatment is test or control. \\
\makecell[r]{\textbf{Domain Knowledge}} & Refers to the domain where the final value and impact of the analysis is assessed, such as business, medicine, sports, military. \\
\makecell[r]{\textbf{Retrospective Study}} & Involves existing data, predicting outcomes which must be hidden during training for strong validation. \\
\makecell[r]{\textbf{Prospective Study}} & Involves predicting and validating against future outcomes. \\
\makecell[r]{\textbf{Artificial Intelligence}} & Big topic with much history and varying meanings and nuances.  Somewhat inseparable from the study of human cognition, since one
important driver of AI is its resemblance to human cognition and ability to perform tasks previously requiring humans.  Autonomous cars and other robots are fairly
clear examples of AI. \\
\makecell[r]{\textbf{Knowledge Graph}} & Nodes and edges, like any graph, but representing entities and relationships. \\
\hline
\end{tabular}
\end{table}

\end{appendices}
